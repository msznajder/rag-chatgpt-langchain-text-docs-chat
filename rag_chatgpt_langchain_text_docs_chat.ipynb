{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f27ac1d-47b4-44d4-bf6e-9e086c01e195",
   "metadata": {},
   "source": [
    "# RAG-based text documents Q&A chat with LangChain and ChatGPT "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8bd958-7b11-4ce2-ae9e-3115be6b699d",
   "metadata": {},
   "source": [
    "RAG stands for retrieval augmented generation and it works by retrieving external documents and using them when executing queries to the LLMs. Using this technique we can ask out language model questions specific for the content of these documents. We will build a simple demo of it where the LLM will answer some questions regarding the set of external text files.\n",
    "\n",
    "We'll use Stanford's CS224 Natural Language Processing with Deep Learning amazing course's syllabus and lectures trascript text files as our external data content we want to ask questions about.\n",
    "\n",
    "In this experiment I used:\n",
    "\n",
    "* Data source: text files\n",
    "* Embeddings: OpenAIEmbeddings\n",
    "* Model: gpt-3.5-turbo\n",
    "* RAG: LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edf207e5-6097-4c9a-a30c-afba18f04345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install openai\n",
    "# !pip install tiktoken\n",
    "# !pip install chromadb\n",
    "# !pip install lark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664912c6-e7cc-4fd5-9cd2-8b409af357c9",
   "metadata": {},
   "source": [
    "## 1. Loading documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45a1eaf-ae87-4245-8ed8-e53ce0ae53ba",
   "metadata": {},
   "source": [
    "We'll start by loading the data we want to ask our LLM about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b8fcf3b-ac8b-4ef4-9c4d-83f1033e751d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "DATA_PATH = './data/01_chatgpt_docs_chat/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bacde24-448f-412e-ba99-2e18a9ac8a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loaders = [\n",
    "    TextLoader(DATA_PATH + \"CS224n_Syllabus.txt\"),\n",
    "    TextLoader(DATA_PATH + \"CS224N_NLP_with_Deep_Learning_Winter_2021_Lecture_1.txt\"),\n",
    "    TextLoader(DATA_PATH + \"CS224N_NLP_with_Deep_Learning_Winter_2021_Lecture_2.txt\"),\n",
    "    TextLoader(DATA_PATH + \"CS224N_NLP_with_Deep_Learning_Winter_2021_Lecture_3.txt\"),\n",
    "    TextLoader(DATA_PATH + \"CS224N_NLP_with_Deep_Learning_Winter_2021_Lecture_4.txt\")\n",
    "]\n",
    "pages = []\n",
    "for loader in loaders:\n",
    "    pages.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a30f7bc8-0905-48cf-acb6-4466d7fa9458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aed2471-f150-4595-a6ca-207d0c7c9166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CS224n: Natural Language Processing with Deep Learning\\nStanford / Winter 2021\\n\\nNatural language processing (NLP) is a crucial part of artificial intelligence (AI), modeling how people share informatio'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0].page_content[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15e5bca8-e02b-46f2-95d8-c510849f98bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': './data/01_chatgpt_docs_chat/CS224n_Syllabus.txt'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd0d164-7c45-40a8-a028-04a8754e5ef2",
   "metadata": {},
   "source": [
    "## 2. Documents splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987dcb3f-f10a-4f5c-b193-dba49a6c8da8",
   "metadata": {},
   "source": [
    "Out next step should be splitting the data. LLMs input context has a limited token length. That is why we need to chunk our input data. \n",
    "\n",
    "Individual chunks, later down the line, will be represented as embeddings vectors which will be selected as input for the model by their semantic similarity to the posted question or problem.\n",
    "\n",
    "We will use one of the Langchain's simplest and essential split method RecursiveCharacterTextSplitter and run it on our loaded PDFs text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "767069af-ba29-4b2f-a7f5-99b0e0142f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb570417-325f-4118-8a20-7777600475ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b08a992c-a140-4e90-b2fc-70a91a30b2f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "439d72ce-eadd-4657-b066-ba0f988b044f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da7444f0-1795-4bb3-8525-03472623a9b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='CS224n: Natural Language Processing with Deep Learning\\nStanford / Winter 2021\\n\\nNatural language processing (NLP) is a crucial part of artificial intelligence (AI), modeling how people share information. In recent years, deep learning approaches have obtained very high performance on many NLP tasks. In this course, students gain a thorough introduction to cutting-edge neural networks for NLP.\\n\\nInstructors:\\nChris Manning\\nJohn Hewitt\\n\\nTA:\\nCourse Coordinator\\nAmelie Byun\\nTeaching Assistants\\nDaniel Do\\nRachel Gardner\\nDavide Giovanardi\\nAlvin Hou\\nPrerna Khullar\\nGita Krishna\\nMegan Leszczynski\\nElissa Li\\nMandy Lu\\nShikhar Murty\\nAkshay Smit\\nDilara Soylu\\nAngelica Sun\\nChris Waites\\nAndrew Wang\\nRui Wang\\nYuyan Wang\\nZihan Wang\\nLingjue Xie\\nRui Yan\\nAnna Yang\\nLauren Zhu\\nLogistics', metadata={'source': './data/01_chatgpt_docs_chat/CS224n_Syllabus.txt'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468eabac-9047-4555-9392-c5f472a121db",
   "metadata": {},
   "source": [
    "## 3. Embeddings and vector storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0300c87c-bee2-47cf-b65b-81706e7f8c17",
   "metadata": {},
   "source": [
    "Like I said above, we will use the generated individual chunks and represent their meaning as embeddings vectors representing the semantic meaning of the chunk of text in the high-dimensional space.\n",
    "\n",
    "Since we are using ChatGPT as our LLM we will use the OpenAIEmbeddings for good fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d92aa68b-bd90-4b99-b016-6e147d02dc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michal/miniconda3/envs/pytorch_env/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746e2d5f-b01e-4b7e-bab1-fd40918809ee",
   "metadata": {},
   "source": [
    "In order to be able to use the generated documents chunks embeddings vector we need to store them in a persistent and easy to access way. Vectorstores do exactly this. It is a vector database that stores our embeddings that will be then used when performing queries using our LLM.\n",
    "\n",
    "Chroma will serve our embeddings storage and retrieval needs pretty well. Its `from_documents` method will also take care of transforming the text chunks into the embeddings form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37328611-7543-4d60-b865-bc8f5f2e9887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove previous version if exist\n",
    "!rm -rf ./data/01_chatgpt_docs_chat/chroma/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64f75379-bc9b-4815-9f13-98b9a0117bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8d6ee50c-06f8-479c-8dac-a72cf6e511f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = DATA_PATH + \"chroma/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "99328c84-cd18-4a65-beb6-fe2507475e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/01_chatgpt_docs_chat/chroma/'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persist_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f408755-42ac-4e6d-ae97-12c8d02cf126",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_db = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "692ce562-99db-4993-a64b-9991ef3b6a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd90833b-8f0a-4e23-8289-5da1176caa06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_db._collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2a48d7-d879-46c4-9e7f-9dd67279a145",
   "metadata": {},
   "source": [
    "Let's use simple embeddings similarity search to answer some question about document by identifying text chunks that potentially could contain information related to the question. No LLMs yet - just simple cosine similarity calculated on question and documents embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae9ea598-dcb3-4e37-a025-ed9e4655dccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Lectures: are on Tuesday/Thursday 4:30-5:50pm Pacific Time (Remote, Zoom link is posted on Canvas).\\nLecture videos for enrolled students: are posted on Canvas (requires login) shortly after each lecture ends. Unfortunately, it is not possible to make these videos viewable by non-enrolled students.\\nPublicly available lecture videos and versions of the course: Complete videos from the 2019 edition are available (free!) on the Stanford Online Hub and on the CS224N YouTube channel. Anyone is welcome to enroll in XCS224N: Natural Language Processing with Deep Learning, the Stanford Artificial Intelligence Professional Program version of this course, throughout the year (medium fee, community TAs and certificate). You can enroll in CS224N via Stanford online in the (northern hemisphere) Autumn to do the course in the Winter (high cost, gives Stanford credit). The lecture slides and assignments are updated online each year as the course progresses. We are happy for anyone to use these resources, but we are happy to get acknowledgements.\\nOffice hours: We've switched to using Nooks for office hours. Information here.\\nContact: Students should ask all course-related questions in the Ed forum (not Piazza), where you will also find announcements. For external enquiries, emergencies, or personal matters that you don't wish to put in a private Ed post, you can email us at cs224n-win2021-staff@lists.stanford.edu.\\n\\nContent\\nWhat is this course about?\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Who is the course instructor?\"\n",
    "docs = chroma_db.similarity_search(question, k=3)\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21d807e7-eaa7-43a7-853d-4f2a70b6f705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gensim word vectors example:\\n[code] [preview] \\tSuggested Readings:\\n\\nEfficient Estimation of Word Representations in Vector Space (original word2vec paper)\\nDistributed Representations of Words and Phrases and their Compositionality (negative sampling paper)\\n\\nAssignment 1 out\\n[code]\\n[preview] \\t\\nThu Jan 14 \\tWord Vectors 2 and Word Window Classification\\n[slides] [notes] \\tSuggested Readings:\\n\\nGloVe: Global Vectors for Word Representation (original GloVe paper)\\nImproving Distributional Similarity with Lessons Learned from Word Embeddings\\nEvaluation methods for unsupervised word embeddings\\n\\nAdditional Readings:\\n\\nA Latent Variable Model Approach to PMI-based Word Embeddings\\nLinear Algebraic Structure of Word Senses, with Applications to Polysemy\\nOn the Dimensionality of Word Embedding\\n\\n\\t\\nFri Jan 15 \\tPython Review Session\\n[code] [preview] \\t10:00am - 11:20am \\t\\t\\nTue Jan 19 \\tBackprop and Neural Networks\\n[slides] [notes] \\tSuggested Readings:\\n\\nmatrix calculus notes\\nReview of differential calculus\\nCS231n notes on network architectures\\nCS231n notes on backprop\\nDerivatives, Backpropagation, and Vectorization\\nLearning Representations by Backpropagating Errors (seminal Rumelhart et al. backpropagation paper)\\n\\nAdditional Readings:\\n\\nYes you should understand backprop\\nNatural Language Processing (Almost) from Scratch\\n\\nAssignment 2 out\\n[code] [handout] \\tAssignment 1 due\\nThu Jan 21 \\tDependency Parsing\\n[slides] [notes]\\n[slides (annotated)] \\tSuggested Readings:'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"In which lecture - give the number and date - are word embeddings described?\"\n",
    "docs = chroma_db.similarity_search(question, k=3)\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "611abba2-73b7-4bea-a103-6cd85590ab0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Credit:\\n    Assignment 1 (6%): Introduction to word vectors\\n    Assignment 2 (12%): Derivatives and implementation of word2vec algorithm\\n    Assignment 3 (12%): Dependency parsing and neural network foundations\\n    Assignment 4 (12%): Neural Machine Translation with sequence-to-sequence, attention, and subwords\\n    Assignment 5 (12%): Self-supervised learning and fine-tuning with Transformers\\nDeadlines: All assignments are due on either a Tuesday or a Thursday before class (i.e. before 4:30pm). All deadlines are listed in the schedule.\\nSubmission: Assignments are submitted via Gradescope. If you need to sign up for a Gradescope account, please use your @stanford.edu email address. Further instructions are given in each assignment handout. Do not email us your assignments.\\nLate start: If the result gives you a higher grade, we will not use your assignment 1 score, and we will give you an assignment grade based on counting each of assignments 2–5 at 13.5%.\\nCollaboration: Study groups are allowed, but students must understand and complete their own assignments, and hand in one assignment per student. If you worked in a group, please put the names of the members of your study group at the top of your assignment. Please ask if you have any questions about the collaboration policy.\\nHonor Code: We expect students to not look at solutions or implementations online. Like all other classes at Stanford, we take the student Honor Code seriously.\\n\\nFinal Project (43%)'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How many assignments are there in the course?\"\n",
    "docs = chroma_db.similarity_search(question, k=3)\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8096470-5afe-4028-86e6-90202f437f8a",
   "metadata": {},
   "source": [
    "It works pretty well. That's the magic of embeddings alone. Just this high-dimensional vector representation of the documents chunks meaning helps to identify the semantic relation between question we pose and the part of the text that can help to answer it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8d81b6-e28d-4caf-a07e-78832da5f63f",
   "metadata": {},
   "source": [
    "## 4. Question answering\n",
    "\n",
    "Retrieval - the R in RAG - is used for output generation in question answering step using `RetrievalQA` chain and loaded target LLM.\n",
    "\n",
    "We start by loading our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0f9d3bf-e2eb-413f-9f96-e78e39e15c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michal/miniconda3/envs/pytorch_env/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79f18803-2817-47fb-9109-8287ae5a5a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michal/miniconda3/envs/pytorch_env/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40df55be-8524-4e0c-8826-d38459fdd474",
   "metadata": {},
   "source": [
    "We then prepare prompt template to fit the model specification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81b63b6d-08dc-4ec5-bf3f-bb3ee62abbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use this context to answer the question. Keep the answer short and concise with up to three sentences. Say you don't know if you don't know the answer.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "896e2b1b-5fe8-4dad-bdd8-34a0e7cf5dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use this context to answer the question. Keep the answer short and concise with up to three sentences. Say you don't know if you don't know the answer.\n",
      "This is an example context.\n",
      "Question: Explain what are Deep Neural Networks in 2-3 sentences.\n",
      "Helpful Answer:\n"
     ]
    }
   ],
   "source": [
    "context = \"This is an example context.\"\n",
    "question = \"Explain what are Deep Neural Networks in 2-3 sentences.\"\n",
    "print(QA_CHAIN_PROMPT.format(context=context, question=question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcb24c2-8e7b-4e56-a31c-ec73f5edd5cc",
   "metadata": {},
   "source": [
    "To combine the LLM with the database, we'll use the `RetrievalQA` chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05b069c7-1bd6-4074-b3ef-60aa7482e407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                       retriever=chroma_db.as_retriever(),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef84a7d8-20be-4821-bf7d-2469d515a973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michal/miniconda3/envs/pytorch_env/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Christopher Manning'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Who is the course instructor?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58fddd7e-8b7c-4715-b59d-1f4d5c0e1983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The course topic is Natural Language Processing with Deep Learning.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is the course topic?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5f20d2b-2f9c-41d9-b194-bc2792b51a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lecture 1\n"
     ]
    }
   ],
   "source": [
    "question = \"In which lecture are word embeddings described? Please return just lecture number.\"\n",
    "result = qa_chain({\"query\": question})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b911ea6-50f9-4a6d-a3f1-1349b1d99d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are five assignments in the course.\n"
     ]
    }
   ],
   "source": [
    "question = \"How many assignments are there in the course?\" \n",
    "result = qa_chain({\"query\": question})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4024bd98-e836-43e4-9eb4-926ceb03a7eb",
   "metadata": {},
   "source": [
    "The final problem that we need to address is making sure the question answering task will have element of continuity between subsequent question to be similar to actual natural conversation. \n",
    "\n",
    "The problem can be visible when we ask a follow up question refering implicitely to the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "414a5c25-68a6-4059-a02e-c14e03d9ea1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Transformers and Pretraining\n",
      "2. Question Answering\n",
      "3. Natural Language Generation\n"
     ]
    }
   ],
   "source": [
    "question = \"What are their topics? Present them as numbered list.\"\n",
    "result = qa_chain({\"query\": question})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8714f586-9dd9-4258-9b24-ff4d0ffa30a7",
   "metadata": {},
   "source": [
    "As we can see in the follow up question the LLM lost the context of the course assignments mentioned in previous question. And that is quite obvious as LLMs are not equiped with dialogue memory. \n",
    "\n",
    "To make the Q&A more natural we will use `ConversationBufferMemory` conversation memory mechanism provided by LangChain. We will also use more complex `ConversationalRetrievalChain` - instead of the original `RetrievalQA` - chain in order to handle the conversation history and feeding it to the LLM with queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a35a8aa1-1478-4085-bf55-de238dd81729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13ea8080-100e-40a6-94ba-b251914d43ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "qa_chain_memory = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=chroma_db.as_retriever(),\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90218b94-927e-434a-9bef-542532cbb5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are five assignments in the course.\n"
     ]
    }
   ],
   "source": [
    "question = \"How many assignments are there in the course?\"\n",
    "result = qa_chain_memory({\"question\": question})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2878e21-5462-4fbd-bc07-25908c8b5939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Introduction to word vectors\n",
      "2. Derivatives and implementation of word2vec algorithm\n",
      "3. Dependency parsing and neural network foundations\n",
      "4. Neural Machine Translation with sequence-to-sequence, attention, and subwords\n",
      "5. Self-supervised learning and fine-tuning with Transformers\n"
     ]
    }
   ],
   "source": [
    "question = \"What are their topics? Present them as numbered list.\"\n",
    "result = qa_chain_memory({\"question\": question})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "caaa9201-f20f-443e-8e4c-6a35899d5fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topic of the third assignment is dependency parsing and neural network foundations.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the third one?\"\n",
    "result = qa_chain_memory({\"question\": question})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef0ae6c-bfde-451e-b3a9-6ab148539a2a",
   "metadata": {},
   "source": [
    "Now - after adding the conversation memory buffer - we can see the follow up question does not loose the context of previous answers and give more details on it keeping the conversation continuit intact.\n",
    "\n",
    "Our RAG text data files Q&A chat is now fully functional and works quite well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8ee782-7eba-46f8-b5bb-26101f0a0958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
