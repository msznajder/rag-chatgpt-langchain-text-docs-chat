{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f27ac1d-47b4-44d4-bf6e-9e086c01e195",
   "metadata": {
    "id": "0f27ac1d-47b4-44d4-bf6e-9e086c01e195"
   },
   "source": [
    "# RAG-based text documents Q&A chat with LangChain, ChatGPT and Mistral 7B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8bd958-7b11-4ce2-ae9e-3115be6b699d",
   "metadata": {
    "id": "cf8bd958-7b11-4ce2-ae9e-3115be6b699d"
   },
   "source": [
    "I have built a simple RAG demo where the LLM answers questions regarding the set of external text files. RAG stands for retrieval augmented generation and it works by retrieving external documents and using them when executing queries to the LLMs. Using this technique we can ask out language model questions specific for the content of these documents. We will build a simple demo of it where the LLM will answer some questions regarding the set of external text files.\n",
    "\n",
    "We will build two versions of this RAG in parallel: one using ChatGPT model and the other using Mistral 7B open-source model. I do it mostly for comparison of both closed-source and open-source solutions for such taks. I should mention that the second approach is especially interesting as it is completely independent and can be run even offline - without calling and kind of API like with ChatGPT\n",
    "\n",
    "We'll use Stanford's CS224 Natural Language Processing with Deep Learning amazing course's syllabus and lectures trascript text files as our external data content we want to ask questions about.\n",
    "\n",
    "In this experiment I used:\n",
    "\n",
    "* Data source: text files\n",
    "* Model 1: gpt-3.5-turbo with OpenAIEmbeddings embeddings\n",
    "* Model 2: Mistral 7B with e5-large embeddings\n",
    "* RAG: LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edf207e5-6097-4c9a-a30c-afba18f04345",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "edf207e5-6097-4c9a-a30c-afba18f04345",
    "outputId": "3066da50-d190-40bf-f034-083ab6efb82f"
   },
   "outputs": [],
   "source": [
    "!pip install accelerate\n",
    "!pip install langchain\n",
    "!pip install openai\n",
    "!pip install langchain-openai\n",
    "!pip install sentence-transformers\n",
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664912c6-e7cc-4fd5-9cd2-8b409af357c9",
   "metadata": {
    "id": "664912c6-e7cc-4fd5-9cd2-8b409af357c9"
   },
   "source": [
    "## 1. Loading documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45a1eaf-ae87-4245-8ed8-e53ce0ae53ba",
   "metadata": {
    "id": "a45a1eaf-ae87-4245-8ed8-e53ce0ae53ba"
   },
   "source": [
    "We'll start by loading the data we want to ask our LLM about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b8fcf3b-ac8b-4ef4-9c4d-83f1033e751d",
   "metadata": {
    "id": "5b8fcf3b-ac8b-4ef4-9c4d-83f1033e751d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "DATA_PATH = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "_Ad1Yi5Wp9gF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "_Ad1Yi5Wp9gF",
    "outputId": "2d28b989-71bc-4ada-b785-6a04b329e4b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bacde24-448f-412e-ba99-2e18a9ac8a08",
   "metadata": {
    "id": "7bacde24-448f-412e-ba99-2e18a9ac8a08"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loaders = [\n",
    "    TextLoader(DATA_PATH + \"CS224n_Syllabus.txt\"),\n",
    "    TextLoader(DATA_PATH + \"CS224N_NLP_with_Deep_Learning_Winter_2021_Lecture_1.txt\"),\n",
    "    TextLoader(DATA_PATH + \"CS224N_NLP_with_Deep_Learning_Winter_2021_Lecture_2.txt\"),\n",
    "    TextLoader(DATA_PATH + \"CS224N_NLP_with_Deep_Learning_Winter_2021_Lecture_3.txt\"),\n",
    "    TextLoader(DATA_PATH + \"CS224N_NLP_with_Deep_Learning_Winter_2021_Lecture_4.txt\")\n",
    "]\n",
    "pages = []\n",
    "for loader in loaders:\n",
    "    pages.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a30f7bc8-0905-48cf-acb6-4466d7fa9458",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a30f7bc8-0905-48cf-acb6-4466d7fa9458",
    "outputId": "1381f106-f13f-4306-8142-f9939f483056"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aed2471-f150-4595-a6ca-207d0c7c9166",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "1aed2471-f150-4595-a6ca-207d0c7c9166",
    "outputId": "1aac1e47-c8e7-4fc2-ade4-2d5ba93002aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CS224n: Natural Language Processing with Deep Learning\\nStanford / Winter 2021\\n\\nNatural language processing (NLP) is a crucial part of artificial intelligence (AI), modeling how people share informatio'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0].page_content[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15e5bca8-e02b-46f2-95d8-c510849f98bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15e5bca8-e02b-46f2-95d8-c510849f98bd",
    "outputId": "6a97d711-b334-40af-c897-9504c47c07cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': './data/CS224n_Syllabus.txt'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd0d164-7c45-40a8-a028-04a8754e5ef2",
   "metadata": {
    "id": "4bd0d164-7c45-40a8-a028-04a8754e5ef2"
   },
   "source": [
    "## 2. Documents splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987dcb3f-f10a-4f5c-b193-dba49a6c8da8",
   "metadata": {
    "id": "987dcb3f-f10a-4f5c-b193-dba49a6c8da8"
   },
   "source": [
    "Out next step should be splitting the data. LLMs input context has a limited token length. That is why we need to chunk our input data.\n",
    "\n",
    "Individual chunks, later down the line, will be represented as embeddings vectors which will be selected as input for the model by their semantic similarity to the posted question or problem.\n",
    "\n",
    "We will use one of the Langchain's simplest and essential split method RecursiveCharacterTextSplitter and run it on our loaded PDFs text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "767069af-ba29-4b2f-a7f5-99b0e0142f4b",
   "metadata": {
    "id": "767069af-ba29-4b2f-a7f5-99b0e0142f4b"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500, # 1000\n",
    "    chunk_overlap = 150 # 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb570417-325f-4118-8a20-7777600475ab",
   "metadata": {
    "id": "cb570417-325f-4118-8a20-7777600475ab"
   },
   "outputs": [],
   "source": [
    "splits = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b08a992c-a140-4e90-b2fc-70a91a30b2f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b08a992c-a140-4e90-b2fc-70a91a30b2f6",
    "outputId": "e776373c-d270-45e5-f2b0-0802fc549d6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "439d72ce-eadd-4657-b066-ba0f988b044f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "439d72ce-eadd-4657-b066-ba0f988b044f",
    "outputId": "5944b3a1-4fd6-4ba8-ddd3-0e1cf27f47eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da7444f0-1795-4bb3-8525-03472623a9b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "da7444f0-1795-4bb3-8525-03472623a9b2",
    "outputId": "efe55ae8-de03-46df-833f-00a0428ba6bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='CS224n: Natural Language Processing with Deep Learning\\nStanford / Winter 2021\\n\\nNatural language processing (NLP) is a crucial part of artificial intelligence (AI), modeling how people share information. In recent years, deep learning approaches have obtained very high performance on many NLP tasks. In this course, students gain a thorough introduction to cutting-edge neural networks for NLP.\\n\\nInstructors:\\nChris Manning\\nJohn Hewitt\\n\\nTA:\\nCourse Coordinator\\nAmelie Byun\\nTeaching Assistants\\nDaniel Do\\nRachel Gardner\\nDavide Giovanardi\\nAlvin Hou\\nPrerna Khullar\\nGita Krishna\\nMegan Leszczynski\\nElissa Li\\nMandy Lu\\nShikhar Murty\\nAkshay Smit\\nDilara Soylu\\nAngelica Sun\\nChris Waites\\nAndrew Wang\\nRui Wang\\nYuyan Wang\\nZihan Wang\\nLingjue Xie\\nRui Yan\\nAnna Yang\\nLauren Zhu\\nLogistics', metadata={'source': './data/CS224n_Syllabus.txt'})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468eabac-9047-4555-9392-c5f472a121db",
   "metadata": {
    "id": "468eabac-9047-4555-9392-c5f472a121db"
   },
   "source": [
    "## 3. Embeddings and vector storage\n",
    "\n",
    "Like I said above, we will use the generated individual chunks and represent their meaning as embeddings vectors representing the semantic meaning of the chunk of text in the high-dimensional space.\n",
    "\n",
    "We need to perform this step separately for both models since they will use different type of embeddings.\n",
    "\n",
    "### 3.1 ChatGPT embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0300c87c-bee2-47cf-b65b-81706e7f8c17",
   "metadata": {
    "id": "0300c87c-bee2-47cf-b65b-81706e7f8c17"
   },
   "source": [
    "Since we are using ChatGPT as our LLM we will use the OpenAIEmbeddings for good fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d92aa68b-bd90-4b99-b016-6e147d02dc05",
   "metadata": {
    "id": "d92aa68b-bd90-4b99-b016-6e147d02dc05"
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "chatgpt_embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746e2d5f-b01e-4b7e-bab1-fd40918809ee",
   "metadata": {
    "id": "746e2d5f-b01e-4b7e-bab1-fd40918809ee"
   },
   "source": [
    "In order to be able to use the generated documents chunks embeddings vector we need to store them in a persistent and easy to access way. Vectorstores do exactly this. It is a vector database that stores our embeddings that will be then used when performing queries using our LLM.\n",
    "\n",
    "Chroma will serve our embeddings storage and retrieval needs pretty well. Its `from_documents` method will also take care of transforming the text chunks into the embeddings form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37328611-7543-4d60-b865-bc8f5f2e9887",
   "metadata": {
    "id": "37328611-7543-4d60-b865-bc8f5f2e9887"
   },
   "outputs": [],
   "source": [
    "# Remove previous version if exist\n",
    "!rm -rf ./data/chroma_chatgpt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64f75379-bc9b-4815-9f13-98b9a0117bca",
   "metadata": {
    "id": "64f75379-bc9b-4815-9f13-98b9a0117bca"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d6ee50c-06f8-479c-8dac-a72cf6e511f8",
   "metadata": {
    "id": "8d6ee50c-06f8-479c-8dac-a72cf6e511f8"
   },
   "outputs": [],
   "source": [
    "chatgpt_persist_directory = DATA_PATH + \"chroma_chatgpt/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99328c84-cd18-4a65-beb6-fe2507475e38",
   "metadata": {
    "id": "99328c84-cd18-4a65-beb6-fe2507475e38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/chroma_chatgpt/'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatgpt_persist_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f408755-42ac-4e6d-ae97-12c8d02cf126",
   "metadata": {
    "id": "1f408755-42ac-4e6d-ae97-12c8d02cf126"
   },
   "outputs": [],
   "source": [
    "chatgpt_chroma_db = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=chatgpt_embedding,\n",
    "    persist_directory=chatgpt_persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "692ce562-99db-4993-a64b-9991ef3b6a8e",
   "metadata": {
    "id": "692ce562-99db-4993-a64b-9991ef3b6a8e"
   },
   "outputs": [],
   "source": [
    "chatgpt_chroma_db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd90833b-8f0a-4e23-8289-5da1176caa06",
   "metadata": {
    "id": "fd90833b-8f0a-4e23-8289-5da1176caa06"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatgpt_chroma_db._collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2a48d7-d879-46c4-9e7f-9dd67279a145",
   "metadata": {
    "id": "2a2a48d7-d879-46c4-9e7f-9dd67279a145"
   },
   "source": [
    "Let's use simple embeddings similarity search to answer some question about document by identifying text chunks that potentially could contain information related to the question. No LLMs yet - just simple cosine similarity calculated on question and documents embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae9ea598-dcb3-4e37-a025-ed9e4655dccb",
   "metadata": {
    "id": "ae9ea598-dcb3-4e37-a025-ed9e4655dccb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Lectures: are on Tuesday/Thursday 4:30-5:50pm Pacific Time (Remote, Zoom link is posted on Canvas).\\nLecture videos for enrolled students: are posted on Canvas (requires login) shortly after each lecture ends. Unfortunately, it is not possible to make these videos viewable by non-enrolled students.\\nPublicly available lecture videos and versions of the course: Complete videos from the 2019 edition are available (free!) on the Stanford Online Hub and on the CS224N YouTube channel. Anyone is welcome to enroll in XCS224N: Natural Language Processing with Deep Learning, the Stanford Artificial Intelligence Professional Program version of this course, throughout the year (medium fee, community TAs and certificate). You can enroll in CS224N via Stanford online in the (northern hemisphere) Autumn to do the course in the Winter (high cost, gives Stanford credit). The lecture slides and assignments are updated online each year as the course progresses. We are happy for anyone to use these resources, but we are happy to get acknowledgements.\\nOffice hours: We've switched to using Nooks for office hours. Information here.\\nContact: Students should ask all course-related questions in the Ed forum (not Piazza), where you will also find announcements. For external enquiries, emergencies, or personal matters that you don't wish to put in a private Ed post, you can email us at cs224n-win2021-staff@lists.stanford.edu.\\n\\nContent\\nWhat is this course about?\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Who is the course instructor?\"\n",
    "docs = chatgpt_chroma_db.similarity_search(question, k=3)\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21d807e7-eaa7-43a7-853d-4f2a70b6f705",
   "metadata": {
    "id": "21d807e7-eaa7-43a7-853d-4f2a70b6f705"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gensim word vectors example:\\n[code] [preview] \\tSuggested Readings:\\n\\nEfficient Estimation of Word Representations in Vector Space (original word2vec paper)\\nDistributed Representations of Words and Phrases and their Compositionality (negative sampling paper)\\n\\nAssignment 1 out\\n[code]\\n[preview] \\t\\nThu Jan 14 \\tWord Vectors 2 and Word Window Classification\\n[slides] [notes] \\tSuggested Readings:\\n\\nGloVe: Global Vectors for Word Representation (original GloVe paper)\\nImproving Distributional Similarity with Lessons Learned from Word Embeddings\\nEvaluation methods for unsupervised word embeddings\\n\\nAdditional Readings:\\n\\nA Latent Variable Model Approach to PMI-based Word Embeddings\\nLinear Algebraic Structure of Word Senses, with Applications to Polysemy\\nOn the Dimensionality of Word Embedding\\n\\n\\t\\nFri Jan 15 \\tPython Review Session\\n[code] [preview] \\t10:00am - 11:20am \\t\\t\\nTue Jan 19 \\tBackprop and Neural Networks\\n[slides] [notes] \\tSuggested Readings:\\n\\nmatrix calculus notes\\nReview of differential calculus\\nCS231n notes on network architectures\\nCS231n notes on backprop\\nDerivatives, Backpropagation, and Vectorization\\nLearning Representations by Backpropagating Errors (seminal Rumelhart et al. backpropagation paper)\\n\\nAdditional Readings:\\n\\nYes you should understand backprop\\nNatural Language Processing (Almost) from Scratch\\n\\nAssignment 2 out\\n[code] [handout] \\tAssignment 1 due\\nThu Jan 21 \\tDependency Parsing\\n[slides] [notes]\\n[slides (annotated)] \\tSuggested Readings:'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"In which lecture - give the number and date - are word embeddings described?\"\n",
    "docs = chatgpt_chroma_db.similarity_search(question, k=3)\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "611abba2-73b7-4bea-a103-6cd85590ab0e",
   "metadata": {
    "id": "611abba2-73b7-4bea-a103-6cd85590ab0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Credit:\\n    Assignment 1 (6%): Introduction to word vectors\\n    Assignment 2 (12%): Derivatives and implementation of word2vec algorithm\\n    Assignment 3 (12%): Dependency parsing and neural network foundations\\n    Assignment 4 (12%): Neural Machine Translation with sequence-to-sequence, attention, and subwords\\n    Assignment 5 (12%): Self-supervised learning and fine-tuning with Transformers\\nDeadlines: All assignments are due on either a Tuesday or a Thursday before class (i.e. before 4:30pm). All deadlines are listed in the schedule.\\nSubmission: Assignments are submitted via Gradescope. If you need to sign up for a Gradescope account, please use your @stanford.edu email address. Further instructions are given in each assignment handout. Do not email us your assignments.\\nLate start: If the result gives you a higher grade, we will not use your assignment 1 score, and we will give you an assignment grade based on counting each of assignments 2–5 at 13.5%.\\nCollaboration: Study groups are allowed, but students must understand and complete their own assignments, and hand in one assignment per student. If you worked in a group, please put the names of the members of your study group at the top of your assignment. Please ask if you have any questions about the collaboration policy.\\nHonor Code: We expect students to not look at solutions or implementations online. Like all other classes at Stanford, we take the student Honor Code seriously.\\n\\nFinal Project (43%)'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How many assignments are there in the course?\"\n",
    "docs = chatgpt_chroma_db.similarity_search(question, k=3)\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8096470-5afe-4028-86e6-90202f437f8a",
   "metadata": {
    "id": "f8096470-5afe-4028-86e6-90202f437f8a"
   },
   "source": [
    "It works pretty well. That's the magic of embeddings alone. Just this high-dimensional vector representation of the documents chunks meaning helps to identify the semantic relation between question we pose and the part of the text that can help to answer it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb072cb7-3aa4-4131-8371-0e2572a7e09c",
   "metadata": {
    "id": "eb072cb7-3aa4-4131-8371-0e2572a7e09c"
   },
   "source": [
    "### 3.2 Mistral 7B embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc862e6c-7191-4c21-91cb-cfed78a3e912",
   "metadata": {
    "id": "cc862e6c-7191-4c21-91cb-cfed78a3e912"
   },
   "source": [
    "We now follow similar steps but this time we prepare embeddings for Mistral 7B.\n",
    "\n",
    "The first major change we will make in order to use the Mistral 7B model instead of ChatGPT and keep using only open-source tools in the process is switching the embeddings. Here we will use open-source e5-large embeddings (\"Text Embeddings by Weakly-Supervised Contrastive Pre-training\", Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, arXiv 2022).\n",
    "\n",
    "Besides changing the embeddings, the steps we follow for generating them are the same.\n",
    "\n",
    "We start with loading new embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cddc563-8a33-4d81-bb88-3415c914cadc",
   "metadata": {
    "id": "8cddc563-8a33-4d81-bb88-3415c914cadc"
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "llama_embedding = HuggingFaceEmbeddings(\n",
    "\n",
    "    model_name = \"intfloat/e5-large\",\n",
    "\n",
    "    model_kwargs={\"device\": device},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14eaaa6",
   "metadata": {
    "id": "b14eaaa6"
   },
   "source": [
    "We create another vector storage to store the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PkAAzri30tmC",
   "metadata": {
    "id": "PkAAzri30tmC"
   },
   "outputs": [],
   "source": [
    "# Remove previous version if exist\n",
    "!rm -rf ./data/chroma_llama/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb057ec",
   "metadata": {
    "id": "ffb057ec"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8df1d02",
   "metadata": {
    "id": "a8df1d02"
   },
   "outputs": [],
   "source": [
    "llama_persist_directory = DATA_PATH + \"chroma_llama/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be6215f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "7be6215f",
    "outputId": "4f8ed832-34b7-4821-9ba7-36c0789f93bd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'./data/chroma_llama/'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_persist_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dac46c6",
   "metadata": {
    "id": "6dac46c6"
   },
   "outputs": [],
   "source": [
    "llama_chroma_db = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=llama_embedding,\n",
    "    persist_directory=llama_persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327374c4",
   "metadata": {
    "id": "327374c4"
   },
   "outputs": [],
   "source": [
    "llama_chroma_db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dab493",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3dab493",
    "outputId": "13636884-e29e-4f59-ad45-15192679c693"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_chroma_db._collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129e6de5",
   "metadata": {
    "id": "129e6de5"
   },
   "source": [
    "Again, let's use simple embeddings similarity search to answer some question with cosine similarity calculated on question and documents embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa76e324",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "aa76e324",
    "outputId": "f1a867b4-12eb-4e4d-d835-4d4562b2f28a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Hi, everybody.\\n\\nWelcome to Stanford's\\nCS224N, also known\\n\\nas Ling284, Natural Language\\nProcessing with Deep Learning.\\n\\nI'm Christopher Manning,\\nand I'm the main instructor\\n\\nfor this class.\\n\\nSo what we hope to do\\ntoday is to dive right in.\\n\\nSo I'm going to spend\\nabout 10 minutes talking\\n\\nabout the course,\\nand then we're going\\n\\nto get straight into\\ncontent for reasons\\n\\nI'll explain in a minute.\\n\\nSo we'll talk about human\\nlanguage and word meaning,\\n\\nI'll then introduce the ideas\\nof the word2vec algorithm\\n\\nfor learning word meaning.\\n\\nAnd then going from there\\nwe'll kind of concretely\\n\\nwork through how you can\\nwork out objective function\\n\\ngradients with respect to\\nthe word2vec algorithm,\\n\\nand say a teeny bit about\\nhow optimization works.\\n\\nAnd then right at\\nthe end of the class\\n\\nI then want to spend\\na little bit of time\\n\\ngiving you a sense of how\\nthese word vectors work,\\n\\nand what you can do with them.\\n\\nSo really the key\\nlearning for today\\n\\nis, I want to give you a sense\\nof how amazing deep learning\\n\\nword vectors are.\\n\\nSo we have this really\\nsurprising result\\n\\nthat word meaning\\ncan be represented,\\n\\nnot perfectly but\\nreally rather well\\n\\nby a large vector\\nof real numbers.\\n\\nAnd that's sort of in a way, a\\ncommonplace of the last decade\\n\\nof deep learning, but it\\nflies in the face of thousands\\n\\nof years of tradition.\\n\\nAnd it's really rather\\nan unexpected result\\n\\nto start focusing on.\\n\\nOK, so quickly what do we\\nhope to teach in this course?\\n\\nSo we've got three\\nprimary goals.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Who is the course instructor?\"\n",
    "docs = llama_chroma_db.similarity_search(question, k=3)\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ae3983",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "12ae3983",
    "outputId": "611a0325-69f2-4a9e-d6f0-42fb14e41ffd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Gensim word vectors example:\\n[code] [preview] \\tSuggested Readings:\\n\\nEfficient Estimation of Word Representations in Vector Space (original word2vec paper)\\nDistributed Representations of Words and Phrases and their Compositionality (negative sampling paper)\\n\\nAssignment 1 out\\n[code]\\n[preview] \\t\\nThu Jan 14 \\tWord Vectors 2 and Word Window Classification\\n[slides] [notes] \\tSuggested Readings:\\n\\nGloVe: Global Vectors for Word Representation (original GloVe paper)\\nImproving Distributional Similarity with Lessons Learned from Word Embeddings\\nEvaluation methods for unsupervised word embeddings\\n\\nAdditional Readings:\\n\\nA Latent Variable Model Approach to PMI-based Word Embeddings\\nLinear Algebraic Structure of Word Senses, with Applications to Polysemy\\nOn the Dimensionality of Word Embedding\\n\\n\\t\\nFri Jan 15 \\tPython Review Session\\n[code] [preview] \\t10:00am - 11:20am \\t\\t\\nTue Jan 19 \\tBackprop and Neural Networks\\n[slides] [notes] \\tSuggested Readings:\\n\\nmatrix calculus notes\\nReview of differential calculus\\nCS231n notes on network architectures\\nCS231n notes on backprop\\nDerivatives, Backpropagation, and Vectorization\\nLearning Representations by Backpropagating Errors (seminal Rumelhart et al. backpropagation paper)\\n\\nAdditional Readings:\\n\\nYes you should understand backprop\\nNatural Language Processing (Almost) from Scratch\\n\\nAssignment 2 out\\n[code] [handout] \\tAssignment 1 due\\nThu Jan 21 \\tDependency Parsing\\n[slides] [notes]\\n[slides (annotated)] \\tSuggested Readings:'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"In which lecture - give the number and date - are word embeddings described?\"\n",
    "docs = llama_chroma_db.similarity_search(question, k=3)\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec314d7f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "ec314d7f",
    "outputId": "f737b266-291a-4f41-c9be-9fffed46be85"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Michael A. Nielsen. Neural Networks and Deep Learning\\nEugene Charniak. Introduction to Deep Learning\\n\\n\\nCoursework\\nAssignments (54%)\\n\\nThere are five weekly assignments, which will improve both your theoretical understanding and your practical skills. All assignments contain both written questions and programming parts. In office hours, TAs may look at students’ code for assignments 1, 2 and 3 but not for assignments 4 and 5.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How many assignments are there in the course?\"\n",
    "docs = llama_chroma_db.similarity_search(question, k=3)\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d160f2",
   "metadata": {
    "id": "89d160f2"
   },
   "source": [
    "Again, the results look reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8d81b6-e28d-4caf-a07e-78832da5f63f",
   "metadata": {
    "id": "6c8d81b6-e28d-4caf-a07e-78832da5f63f"
   },
   "source": [
    "## 4. Question answering RAG chat - simple case\n",
    "\n",
    "Retrieval - the R in RAG - is used for output generation in question answering step using `RetrievalQA` chain and loaded target LLM.\n",
    "\n",
    "We build here the most simple comversational RAG. Later on we will expand it with memory unit to make it more human-like.\n",
    "\n",
    "The steps - besides the embeddings, embeddings vector storage and model used - are mostly the same. What we will focus here is rather the outputs and their comparison.\n",
    "\n",
    "\n",
    "\n",
    "### 4.1 Simple RAG with ChatGPT\n",
    "\n",
    "We start by loading our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0f9d3bf-e2eb-413f-9f96-e78e39e15c06",
   "metadata": {
    "id": "b0f9d3bf-e2eb-413f-9f96-e78e39e15c06"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "chatgpt_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40df55be-8524-4e0c-8826-d38459fdd474",
   "metadata": {
    "id": "40df55be-8524-4e0c-8826-d38459fdd474"
   },
   "source": [
    "We then prepare prompt template to fit the model specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81b63b6d-08dc-4ec5-bf3f-bb3ee62abbdc",
   "metadata": {
    "id": "81b63b6d-08dc-4ec5-bf3f-bb3ee62abbdc"
   },
   "outputs": [],
   "source": [
    "chatgpt_template = \"\"\"Use this context to answer the question. Keep the answer short and concise with up to three sentences. Say you don't know if you don't know the answer.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "CB6ye0c0dAT0",
   "metadata": {
    "id": "CB6ye0c0dAT0"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "CHATGPT_QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"], template=chatgpt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "896e2b1b-5fe8-4dad-bdd8-34a0e7cf5dd2",
   "metadata": {
    "id": "896e2b1b-5fe8-4dad-bdd8-34a0e7cf5dd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use this context to answer the question. Keep the answer short and concise with up to three sentences. Say you don't know if you don't know the answer.\n",
      "This is an example context.\n",
      "Question: Explain what are Deep Neural Networks in 2-3 sentences.\n",
      "Helpful Answer:\n"
     ]
    }
   ],
   "source": [
    "context = \"This is an example context.\"\n",
    "question = \"Explain what are Deep Neural Networks in 2-3 sentences.\"\n",
    "print(CHATGPT_QA_CHAIN_PROMPT.format(context=context, question=question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcb24c2-8e7b-4e56-a31c-ec73f5edd5cc",
   "metadata": {
    "id": "cdcb24c2-8e7b-4e56-a31c-ec73f5edd5cc"
   },
   "source": [
    "To combine the LLM with the database, we'll use the `RetrievalQA` chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05b069c7-1bd6-4074-b3ef-60aa7482e407",
   "metadata": {
    "id": "05b069c7-1bd6-4074-b3ef-60aa7482e407"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "chatgpt_qa_chain = RetrievalQA.from_chain_type(chatgpt_llm,\n",
    "                                               retriever=chatgpt_chroma_db.as_retriever(),\n",
    "                                               return_source_documents=True,\n",
    "                                               chain_type_kwargs={\"prompt\": CHATGPT_QA_CHAIN_PROMPT})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef84a7d8-20be-4821-bf7d-2469d515a973",
   "metadata": {
    "id": "ef84a7d8-20be-4821-bf7d-2469d515a973"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Christopher Manning'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Who is the course instructor?\"\n",
    "result = chatgpt_qa_chain.invoke({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5f20d2b-2f9c-41d9-b194-bc2792b51a1a",
   "metadata": {
    "id": "f5f20d2b-2f9c-41d9-b194-bc2792b51a1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lecture 1\n"
     ]
    }
   ],
   "source": [
    "question = \"In which lecture are word embeddings described? Please return just lecture number.\"\n",
    "result = chatgpt_qa_chain.invoke({\"query\": question})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b911ea6-50f9-4a6d-a3f1-1349b1d99d9e",
   "metadata": {
    "id": "9b911ea6-50f9-4a6d-a3f1-1349b1d99d9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are five assignments in the course.\n"
     ]
    }
   ],
   "source": [
    "question = \"How many assignments are there in the course?\"\n",
    "result = chatgpt_qa_chain.invoke({\"query\": question})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c031c0e3-27b8-4abf-bc21-f1c318054aaf",
   "metadata": {},
   "source": [
    "All the answers are really short and crisp. Nothing to complain about - besides the fact that geenrating them takes sending our data to the OpenAI API endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552c2cf8-836d-40f1-a2e9-134a8733119a",
   "metadata": {
    "id": "552c2cf8-836d-40f1-a2e9-134a8733119a"
   },
   "source": [
    "### 4.2 Simple RAG with Llama 2\n",
    "\n",
    "We now follow similar steps to build a Q&A chat but this time with Llama 2.\n",
    "\n",
    "The main difference here is that insteaf of connecting to OpenAI API we simply load trained model checkpoint.\n",
    "\n",
    "This solution is 100% open-source and self hosted - meaning that at no point you are sending any of your data outside your environment.\n",
    "\n",
    "We start by loading a model from Hugging Face hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520cad39-7709-4a35-ac30-f8315955f692",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "16a3740408274dc88a806f5e21cc4896",
      "352e172d261c48c7ab0e0f62a8d59f81",
      "c952e14cba274b3ea4afed7cc05ceae3",
      "dea0fb4361f341088d2a144a313e4d3b",
      "e33a42eba78b40a09d4472f236a39292",
      "be8cf36ebd5e4722842db6f5ed272aad",
      "739fb8d344004a2eb93a55659a8702f9",
      "e3b3cb63c58d4874a997aba851ad7ea5",
      "272f7a29844c4619a6882aa4474f6972",
      "88f2201d24424c58842f727e717bd1a8",
      "7533106aea6c4aa9bed8b464327c5d05"
     ]
    },
    "id": "520cad39-7709-4a35-ac30-f8315955f692",
    "outputId": "fd8705fc-d928-44b3-ac66-25b5d9712d52"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a3740408274dc88a806f5e21cc4896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "generation_config.max_new_tokens = 256\n",
    "generation_config.temperature = 0.0001\n",
    "generation_config.top_p = 0.75\n",
    "generation_config.do_sample = True\n",
    "generation_config.repetition_penalty = 1.15\n",
    "\n",
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    device=device,\n",
    "    tokenizer=tokenizer,\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "llama_llm = HuggingFacePipeline(pipeline=text_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44d7604-d10d-4e06-b47b-7abddd2f9920",
   "metadata": {},
   "source": [
    "We then prepare the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zYqS-WNddL_A",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zYqS-WNddL_A",
    "outputId": "ae3b5946-147b-4e96-dd9a-0a9bcbc7d12c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] You are a helpful assistant. Your task is to answer the following question using below context. Use up to four sentences. Keep the answer concise. Say you don't know if you don't know the answer.\n",
      "\n",
      "Context: xxx\n",
      "\n",
      "Question: yyy[/INST]\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "llama_template = \"\"\"<s>[INST] You are a helpful assistant. Your task is to answer the following question using below context. Use up to four sentences. Keep the answer concise. Say you don't know if you don't know the answer.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}[/INST]\n",
    "</s>\"\"\"\n",
    "print(llama_template.format(context=\"xxx\", question=\"yyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2COxEKWzcxdC",
   "metadata": {
    "id": "2COxEKWzcxdC"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "LLAMA_QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"], template=llama_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3b35ce-7615-4f4d-9d56-787c052546de",
   "metadata": {},
   "source": [
    "And we set up the `RetrievalQA` specifically for Mistral 7B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de83697a",
   "metadata": {
    "id": "de83697a"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llama_qa_chain = RetrievalQA.from_chain_type(llama_llm,\n",
    "                                             retriever=llama_chroma_db.as_retriever(),\n",
    "                                             return_source_documents=True,\n",
    "                                             chain_type_kwargs={\"prompt\": LLAMA_QA_CHAIN_PROMPT})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eae2ce-07fd-4eff-822a-792725778766",
   "metadata": {},
   "source": [
    "Finally, we can now query the model about our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "456c2b45",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "456c2b45",
    "outputId": "1e80d074-a27b-49ff-d0fc-22a7f9cbc033"
   },
   "outputs": [],
   "source": [
    "question = \"Who is the course instructor?\"\n",
    "result = llama_qa_chain.invoke({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yQ7AT8qvTFUN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yQ7AT8qvTFUN",
    "outputId": "ac694239-00ac-4de1-a1d9-24658e1ae213"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The course instructor is Christopher Manning. He is the main instructor for Stanford's CS224N, also known as Ling284, Natural Language Processing with Deep Learning.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"result\"].split(\"</s>\")[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a6be6671",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6be6671",
    "outputId": "7c97daf4-9a46-4010-9b4d-722552143c78"
   },
   "outputs": [],
   "source": [
    "question = \"In which lecture - give the number and date - are word embeddings described?\"\n",
    "result = llama_qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563u-jrfTJIi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "563u-jrfTJIi",
    "outputId": "1f8645f5-d6b3-42d2-ab71-2e6f8de88144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the first lecture on January 14, 2021, Chris Manning introduces the idea of word embeddings using the word2vec algorithm. This lecture sets the foundation for understanding word representations as vectors.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"result\"].split(\"</s>\")[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f408a94b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f408a94b",
    "outputId": "41082ef7-b7c1-410f-c07f-a763a61a2eea"
   },
   "outputs": [],
   "source": [
    "question = \"How many assignments are there in the course?\"\n",
    "result = llama_qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QJe0l7MpTMoo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QJe0l7MpTMoo",
    "outputId": "ccc559c2-64f6-42d1-c00f-9718fafa95c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of five assignments in the course, worth 54% of the final grade. The first assignment was due recently, and the second assignment has been released. Late submission of assignments is permitted, but it may lower the overall grade. Collaboration is allowed in study groups, but each student must submit their own assignment. The final project makes up the remaining 43% of the grade.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"result\"].split(\"</s>\")[-1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c43b130-b7a3-4291-97e7-57fe7e20d284",
   "metadata": {
    "id": "1c43b130-b7a3-4291-97e7-57fe7e20d284"
   },
   "source": [
    "Now, we have answers for our data specific questions coming from both ChatGPT and Llama 2 RAGs.\n",
    "\n",
    "And both of them answer correctly all the answers. The only difference is the language style between the models but aside from that relatively small model like Mistral 7B works very well in such RAG setting.\n",
    "\n",
    "And as a plus, all the data stays within environment. Here it does not matter, but imagine setting where we would create RAG chat based on some medical instituion patients data or some contracts details from consulting firms etc. Open-source Llama 2 based solution is in this aspect very simple and secure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb09659-94b3-4c3e-83e1-c1a680a94190",
   "metadata": {
    "id": "bfb09659-94b3-4c3e-83e1-c1a680a94190"
   },
   "source": [
    "## 5. Question answering RAG chat - robust case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4024bd98-e836-43e4-9eb4-926ceb03a7eb",
   "metadata": {
    "id": "4024bd98-e836-43e4-9eb4-926ceb03a7eb"
   },
   "source": [
    "The final problem that we need to address is making sure the question answering task will have element of continuity between subsequent question to be similar to actual natural conversation.\n",
    "\n",
    "### 5.1 Robust RAG with ChatGPT\n",
    "\n",
    "The problem can be visible when we ask a follow up question refering implicitely to the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "414a5c25-68a6-4059-a02e-c14e03d9ea1a",
   "metadata": {
    "id": "414a5c25-68a6-4059-a02e-c14e03d9ea1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Structure of sentences and conveying meaning in human language.\n",
      "2. Introduction to PyTorch framework for deep learning.\n",
      "3. Final project choices and considerations.\n"
     ]
    }
   ],
   "source": [
    "question = \"What are their topics? Present them as numbered list.\"\n",
    "result = chatgpt_qa_chain.invoke({\"query\": question})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8714f586-9dd9-4258-9b24-ff4d0ffa30a7",
   "metadata": {
    "id": "8714f586-9dd9-4258-9b24-ff4d0ffa30a7"
   },
   "source": [
    "As we can see in the follow up question the LLM lost the context of the course assignments mentioned in previous question. And that is quite obvious as LLMs are not equiped with dialogue memory.\n",
    "\n",
    "To make the Q&A more natural we will use `ConversationBufferMemory` conversation memory mechanism provided by LangChain. We will also use more complex `ConversationalRetrievalChain` - instead of the original `RetrievalQA` - chain in order to handle the conversation history and feeding it to the LLM with queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a35a8aa1-1478-4085-bf55-de238dd81729",
   "metadata": {
    "id": "a35a8aa1-1478-4085-bf55-de238dd81729"
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "chatgpt_memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13ea8080-100e-40a6-94ba-b251914d43ef",
   "metadata": {
    "id": "13ea8080-100e-40a6-94ba-b251914d43ef"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "chatgpt_qa_chain_memory = ConversationalRetrievalChain.from_llm(\n",
    "    chatgpt_llm,\n",
    "    retriever=chatgpt_chroma_db.as_retriever(),\n",
    "    memory=chatgpt_memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": CHATGPT_QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "90218b94-927e-434a-9bef-542532cbb5c4",
   "metadata": {
    "id": "90218b94-927e-434a-9bef-542532cbb5c4"
   },
   "outputs": [],
   "source": [
    "question = \"How many assignments are there in the course?\"\n",
    "result = chatgpt_qa_chain_memory({\"question\": question})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a2878e21-5462-4fbd-bc07-25908c8b5939",
   "metadata": {
    "id": "a2878e21-5462-4fbd-bc07-25908c8b5939"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Introduction to word vectors\n",
      "2. Derivatives and implementation of word2vec algorithm\n",
      "3. Dependency parsing and neural network foundations\n",
      "4. Neural Machine Translation with sequence-to-sequence, attention, and subwords\n",
      "5. Self-supervised learning and fine-tuning with Transformers\n"
     ]
    }
   ],
   "source": [
    "question = \"What are their topics? Present them as numbered list.\"\n",
    "result = chatgpt_qa_chain_memory({\"question\": question})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "caaa9201-f20f-443e-8e4c-6a35899d5fdb",
   "metadata": {
    "id": "caaa9201-f20f-443e-8e4c-6a35899d5fdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topic of the third assignment in the course is Dependency parsing and neural network foundations.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the third one?\"\n",
    "result = chatgpt_qa_chain_memory({\"question\": question})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef0ae6c-bfde-451e-b3a9-6ab148539a2a",
   "metadata": {
    "id": "2ef0ae6c-bfde-451e-b3a9-6ab148539a2a"
   },
   "source": [
    "Now - after adding the conversation memory buffer - we can see that not only the answers are correct but also the follow up question does not loose the context of previous answers and give more details on it keeping the conversation continuit intact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e7860b-4a38-48d5-83e5-3732a489f8e2",
   "metadata": {
    "id": "f6e7860b-4a38-48d5-83e5-3732a489f8e2"
   },
   "source": [
    "### 5.2 Robust RAG with Llama 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bad6767",
   "metadata": {
    "id": "7bad6767"
   },
   "source": [
    "The same problem occurs in case of Llama 2 based RAG where the context of previously asked questions is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1d9afd05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1d9afd05",
    "outputId": "005471cf-143c-4851-a224-130ca2e81c8d"
   },
   "outputs": [],
   "source": [
    "question = \"What are their topics? Present them as numbered list.\"\n",
    "result = llama_qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SbeGhA-ITQrP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SbeGhA-ITQrP",
    "outputId": "c3d58b70-ed16-4434-9082-57b483985de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Math details of neural network learning, including working out gradients by hand and the backpropagation algorithm\n",
      "2. Linguistics and natural language processing, specifically dependency parsing\n",
      "3. Syntactic structure of languages, introducing constituency and dependency\n",
      "4. Dependency grammars and dependency treebanks\n",
      "5. Building natural language processing systems, discussing transition-based dependency parsing\n",
      "6. Developing a simple and highly effective neural dependency parser\n",
      "7. Assignments, including information about submission deadlines and discount options.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"result\"].split(\"</s>\")[-1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6657f7c5",
   "metadata": {
    "id": "6657f7c5"
   },
   "source": [
    "Again, to make the Q&A more natural we will use `ConversationBufferMemory` conversation memory mechanism and more complex `ConversationalRetrievalChain` chain provided by LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f1d1b8",
   "metadata": {
    "id": "71f1d1b8"
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "llama_memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2badfb4",
   "metadata": {
    "id": "d2badfb4"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "llama_qa_chain_memory = ConversationalRetrievalChain.from_llm(\n",
    "    llama_llm,\n",
    "    retriever=llama_chroma_db.as_retriever(),\n",
    "    memory=llama_memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": LLAMA_QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8493653c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8493653c",
    "outputId": "027cc8f7-af79-4ddd-d541-265f6dc1ce15"
   },
   "outputs": [],
   "source": [
    "question = \"How many assignments are there in the course?\"\n",
    "result = llama_qa_chain_memory({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MzSppjjJTTNw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MzSppjjJTTNw",
    "outputId": "ad9dd718-8fdc-4995-b747-a6bee7379062"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of five assignments in the course, worth 54% of the final grade. The first assignment was due recently, and the second assignment has been released. Late submission of assignments is permitted, but it may lower the overall grade. Collaboration is allowed in study groups, but each student must submit their own assignment. The final project makes up the remaining 43% of the grade.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"answer\"].split(\"</s>\")[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "288b1b8a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "288b1b8a",
    "outputId": "6770bd3e-17f1-4d14-91cd-9a4e3bfa575c"
   },
   "outputs": [],
   "source": [
    "question = \"What are their topics? List them as numbered list.\"\n",
    "result = llama_qa_chain_memory({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RFhpgWoCTUhq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RFhpgWoCTUhq",
    "outputId": "1db2cabe-f169-47ad-a006-f0a24e40f5d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The five assignments cover various topics related to natural language processing and deep learning. Here's a brief overview of each assignment:\n",
      "\n",
      "1. Assignment 1: Introduction to word vectors\n",
      "2. Assignment 2: Derivatives and implementation of word2vec algorithm\n",
      "3. Assignment 3: Dependency parsing and neural network foundations\n",
      "4. Assignment 4: Neural Machine Translation with sequence-to-sequence, attention, and subwords\n",
      "5. Assignment 5: Self-supervised learning and fine-tuning with Transformers\n",
      "\n",
      "These assignments aim to improve both theoretical understanding and practical skills, with both written questions and programming parts. Office hours are available for assistance, and submissions are made via Gradescope. Late submissions may affect the overall grade, while collaboration is allowed within study groups.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"answer\"].split(\"</s>\")[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "282e35b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "282e35b6",
    "outputId": "df0f4480-8150-4240-84f6-9811305b42ae"
   },
   "outputs": [],
   "source": [
    "question = \"What is the third one?\"\n",
    "result = llama_qa_chain_memory({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4HdmPlTXa7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dd4HdmPlTXa7",
    "outputId": "372aa046-934f-4e72-8611-390fca5e8b81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The third assignment is called \"Dependency parsing and neural network foundations.\" It focuses on dependency parsing, which is a common technique used in natural language processing to analyze the grammatical structure of sentences. Additionally, it covers the basics of neural networks and their foundational concepts.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"answer\"].split(\"</s>\")[-1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d8867e",
   "metadata": {
    "id": "a1d8867e"
   },
   "source": [
    "Again, after adding the conversation memory buffer - we can see that besides all answer being factually correct, the follow up question does not loose the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd36f0e-8e2a-459f-9d24-b34157ade6a1",
   "metadata": {
    "id": "fcd36f0e-8e2a-459f-9d24-b34157ade6a1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c19ef18-ba65-48d9-bfce-09ad13988d57",
   "metadata": {
    "id": "5c19ef18-ba65-48d9-bfce-09ad13988d57"
   },
   "source": [
    "Both our RAG text data files Q&A chats are now fully functional and work quite well.\n",
    "\n",
    "This experiment shows that - with tools like LangChain - open-source, free to use and self-hosted model like Llama 7B can offer performance comparable to that of the best commercial models like ChatGPT while you do not need to share your data in any way with APIs.\n",
    "\n",
    "Of course this project is very simple I will probably explore this topic with follow-up experiment, applying similar setup to more realistic high-volume and high-complexity body of documents and data. Stay tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202e79ee-3d73-46f5-b5f9-893922a8fdb8",
   "metadata": {
    "id": "202e79ee-3d73-46f5-b5f9-893922a8fdb8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "16a3740408274dc88a806f5e21cc4896": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_352e172d261c48c7ab0e0f62a8d59f81",
       "IPY_MODEL_c952e14cba274b3ea4afed7cc05ceae3",
       "IPY_MODEL_dea0fb4361f341088d2a144a313e4d3b"
      ],
      "layout": "IPY_MODEL_e33a42eba78b40a09d4472f236a39292"
     }
    },
    "272f7a29844c4619a6882aa4474f6972": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "352e172d261c48c7ab0e0f62a8d59f81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be8cf36ebd5e4722842db6f5ed272aad",
      "placeholder": "​",
      "style": "IPY_MODEL_739fb8d344004a2eb93a55659a8702f9",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "739fb8d344004a2eb93a55659a8702f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7533106aea6c4aa9bed8b464327c5d05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "88f2201d24424c58842f727e717bd1a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be8cf36ebd5e4722842db6f5ed272aad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c952e14cba274b3ea4afed7cc05ceae3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e3b3cb63c58d4874a997aba851ad7ea5",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_272f7a29844c4619a6882aa4474f6972",
      "value": 3
     }
    },
    "dea0fb4361f341088d2a144a313e4d3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_88f2201d24424c58842f727e717bd1a8",
      "placeholder": "​",
      "style": "IPY_MODEL_7533106aea6c4aa9bed8b464327c5d05",
      "value": " 3/3 [00:05&lt;00:00,  1.82s/it]"
     }
    },
    "e33a42eba78b40a09d4472f236a39292": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3b3cb63c58d4874a997aba851ad7ea5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
